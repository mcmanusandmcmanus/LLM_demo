This app hosts a Hugging Face causal language model (default distilgpt2, swappable via MODEL_NAME) behind FastAPI /generate. Default decoding: max_new_tokens=128, temperature=0.7, top_p=0.95, top_k=50, do_sample=True, device_map=auto, seed=42. UI lives at src/static/index.html; Django alternative in llm_site/. Fine-tuning uses src/train.py (HF Trainer) on plain text files; eval is perplexity/ROUGE-L via src/eval.py. No custom fine-tuned weights are bundled—only tiny sample text splits (5/2/3 lines) in data/sample/.

LLM Summary

Base: Hugging Face causal LM (default distilgpt2, swappable via MODEL_NAME) with text-generation pipeline and do_sample=True; seed set to 42 for reproducibility (src/llm_service.py).
Default decoding: max_new_tokens=128, temperature=0.7, top_p=0.95, top_k=50, device_map=auto (src/config.py, src/llm_service.py).
API/UI: FastAPI at /generate (JSON: prompt, optional decoding params) and /health; serves a playground UI from src/static/index.html (src/main.py, src/static/index.html).
Training: src/train.py fine-tunes a causal LM on plain text files using HF Trainer; defaults: 1 epoch, batch size 2, block size 128, LR 5e-5, eval every 50 steps, save every 200 (src/train.py).
Eval: src/eval.py computes perplexity and ROUGE-L on text files; no precision/recall/F1 by default.
Data: only tiny sample splits in data/sample/ (5 train / 2 val / 3 test lines); no real fine-tuned checkpoint included.
Prompt-ready blurb


I am intrigued. Tell me more about use cases for this LLM. This is an LLM correct?


Yes—this project is an LLM-based stack (Hugging Face causal LM) with FastAPI/Django frontends. Practical use cases for your structured CAD/NIBRS data (once you template or otherwise normalize it):

Incident narratives on the fly: turn structured rows into readable summaries for reports/briefings; enforce code-to-text mappings to avoid hallucination.
Question answering over incidents: “Show burglary calls in precinct X after 10pm last month”; use retrieval/text-to-SQL to keep answers grounded in the table.
Classification/flagging: route/triage calls, flag duplicates, detect missing/invalid fields; here you’d use precision/recall/F1.
Pattern surfacing: summarize trends (“vehicle thefts week-over-week”), highlight anomalies, or draft shift-change bullet points.
Policy/compliance checks: given an incident + disposition, flag if required steps seem missing (needs carefully curated rules/examples).
Redaction/normalization: detect and mask PII in free-text fields (if you later add narratives); standardize agency codes/locations.
Cautions: keep prompts/outputs grounded in actual row data to prevent invented details; add guardrails/rules for sensitive domains; start with small, well-sampled subsets to validate quality before scaling. If you want, I can suggest a minimal pipeline for one of these (e.g., templated narrative generation or QA via text-to-SQL).


Here are deeper, practical outlines for each use case, with data prep, modeling approach, metrics, and guardrails:

Incident narratives on the fly

Data prep: Map codes to human-readable labels (lookup tables), template the 25/13 columns into concise sentences; strip PII.
Modeling: Start with templated generation; fine-tune a small LM on template → richer narrative pairs if you can author better text. Keep copy-from-input constraints (don’t invent fields).
Metrics: ROUGE-L vs. held-out templated references; human spot-check for accuracy (no hallucinated codes/locations).
Guardrails: Hard-replace any model-generated field values with the source row values; cap length; disallow freeform field changes.
Question answering over incidents (table grounding)

Data prep: Normalize time/geo fields; index rows for retrieval; optionally expose a SQL view.
Modeling: Prefer text-to-SQL (or retrieval + rule-based filtering) over freeform generation. A small LM can translate “burglary in precinct X after 10pm last month” into a SQL filter; execute and summarize results.
Metrics: Execution accuracy or exact match on a labeled text-to-SQL set; downstream precision/recall on returned rows.
Guardrails: Reject/clarify ambiguous queries; whitelist columns/clauses; limit row counts and date ranges.
Classification/flagging (triage, duplicates, missing/invalid fields)

Data prep: Label a modest, clean set (few hundred–few thousand examples) for each class/flag; balance classes.
Modeling: Use a lightweight classifier head or prompt-based zero/low-shot; fine-tune with LoRA if needed. For dedupe, combine blocking keys + embedding similarity.
Metrics: Precision/recall/F1 per class; ROC-AUC where applicable; manual QA on borderline cases.
Guardrails: Confidence thresholds; human-in-the-loop for low-confidence flags; audit logs.
Pattern surfacing (trends, anomalies, briefings)

Data prep: Aggregate by time/geo/offense; compute baselines and deltas.
Modeling: Use statistical aggregation first, then have the LM narrate the already-computed stats.
Metrics: Accuracy of reported stats (should be exact); human readability ratings.
Guardrails: Never let the model compute stats—feed it the numbers; include source filters in the prompt.
Policy/compliance checks

Data prep: Encode policies as checklists keyed by offense/disposition; add a small labeled set of “compliant/non-compliant” examples if possible.
Modeling: Rule-first (deterministic checks), optionally LM to explain/justify or to handle fuzzy cases.
Metrics: Precision/recall/F1 on a curated eval set; false-positive/false-negative review.
Guardrails: Show which policy rules triggered; avoid blocking workflows without human review.
Redaction/normalization (if narratives are added later)

Data prep: PII patterns (names, phones, addresses) and code dictionaries.
Modeling: Regex + deterministic rules for PII; LM only as a backstop. Code normalization via lookups, not generation.
Metrics: PII recall (don’t miss); acceptable precision; spot-check leaks.
Guardrails: Prefer rule-based masking; log redaction actions.
General operational tips

Start small: stratified slice (e.g., 100k rows) to validate pipelines and metrics before scaling.
Grounding: Always bind outputs to source fields; avoid freeform generation of factual fields.
Evaluation sets: Keep a fixed, versioned eval set per use case (narrative ROUGE, QA exec accuracy, classification F1).
Deployment: Quantize for CPU if budget-limited; cache lookups; set timeouts and max tokens.
If you pick one use case to start (e.g., templated narratives or text-to-SQL QA), I can draft a minimal pipeline (data prep → model approach → eval → rollout) tailored to it.